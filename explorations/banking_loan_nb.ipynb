{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24751abf-d4af-4c37-9c0e-c1c90e821a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Capa Bronze: Carga inicial de datos crudos\n",
    "# -------------------------------------------------------\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Leer la tabla original\n",
    "df_bronze = spark.sql(\"SELECT * FROM bank_loan.default.bank_personal_loan\")\n",
    "\n",
    "# Limpiar nombres de columnas: minúsculas y espacios → _\n",
    "df_bronze = df_bronze.toDF(*[\n",
    "    c.strip().lower().replace(\" \", \"_\") for c in df_bronze.columns\n",
    "])\n",
    "\n",
    "# Guardar como tabla Delta en la capa Bronze\n",
    "# df_bronze.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bank_loan.default.bank_personal_loan_bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f981bed-a091-4bcc-8640-c97b64aa85a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n>>> 1. TAMAÑO Y ESTRUCTURA <<<\\n\")\n",
    "df_count = df_bronze.count()\n",
    "df_shape = spark.createDataFrame([(df_count, len(df_bronze.columns))], [\"num_rows\", \"num_columns\"])\n",
    "display(df_shape)\n",
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68fc480e-1e8d-4a02-84db-caffa0f5d0b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze_pandas = df_bronze.toPandas()\n",
    "df_bronze_pandas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df3ca084-748d-47b1-9d3b-2d09c1669d10",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759612273881}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a74828-7e78-4f0c-b84f-784676750f62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_silver = spark.read.table(\"bank_loan.default.etl_bank_loan_bronze\")\n",
    "print(df_silver.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbaa0c2-19e8-428a-a9f2-64849b139406",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759633570803}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Capa Silver: limpieza, transformación y escalado manual\n",
    "# -------------------------------------------------------\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# -------------------------\n",
    "# 0. Leer la tabla Bronze\n",
    "# -------------------------\n",
    "df_silver = spark.read.table(\"bank_loan.default.etl_bank_loan_bronze\")\n",
    "\n",
    "# -------------------------\n",
    "# 1. Convertir columnas ID y ZIP a string\n",
    "# -------------------------\n",
    "for c in [\"id\", \"zip_code\"]:\n",
    "    df_silver = df_silver.withColumn(c, F.col(c).cast(\"string\"))\n",
    "\n",
    "# -------------------------\n",
    "# 2. Convertir columnas numéricas bigint/int a double\n",
    "# -------------------------\n",
    "cols = [c for c, t in df_silver.dtypes if t in (\"bigint\", \"int\")]\n",
    "for c in cols:\n",
    "    df_silver = df_silver.withColumn(c, F.col(c).cast(\"double\"))\n",
    "\n",
    "# -------------------------\n",
    "# 3. Función UDF para convertir fracciones a float\n",
    "# -------------------------\n",
    "def frac_to_double(s):\n",
    "    try:\n",
    "        if s is None:\n",
    "            return None\n",
    "        if '/' in s:\n",
    "            num, denom = s.split('/')\n",
    "            return float(num) / float(denom)\n",
    "        else:\n",
    "            return float(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "frac_to_double_udf = F.udf(frac_to_double, DoubleType())\n",
    "df_silver = df_silver.withColumn(\"ccavg\", frac_to_double_udf(F.col(\"ccavg\")))\n",
    "\n",
    "# -------------------------\n",
    "# 4. Rellenar valores nulos con la mediana\n",
    "# -------------------------\n",
    "for col in [\"age\", \"income\", \"ccavg\"]:\n",
    "    median_val = df_silver.approxQuantile(col, [0.5], 0.01)[0]\n",
    "    df_silver = df_silver.fillna({col: median_val})\n",
    "\n",
    "# -------------------------\n",
    "# 5. Escalado manual (z-score) de columnas numéricas\n",
    "# -------------------------\n",
    "\n",
    "# Columnas numéricas continuas que sí queremos escalar\n",
    "numeric_cols = [c for c, t in df_silver.dtypes if t == \"double\"]\n",
    "string_cols = [\"id\", \"zip_code\"]\n",
    "\n",
    "# Variables que NO queremos escalar\n",
    "not_scaled_cols = [\n",
    "    'education',\n",
    "    'family',\n",
    "    'personal_loan',\n",
    "    'securities_account',\t\n",
    "    'cd_account',\t\n",
    "    'online',\t\n",
    "    'creditcard'\n",
    "]\n",
    "\n",
    "# Filtrar solo las numéricas continuas\n",
    "numeric_cols_to_scale = [c for c in numeric_cols if c not in not_scaled_cols]\n",
    "\n",
    "\n",
    "scaled_cols = []\n",
    "\n",
    "for c in numeric_cols_to_scale:\n",
    "    stats = df_silver.select(\n",
    "        F.mean(F.col(c)).alias(\"mean\"),\n",
    "        F.stddev(F.col(c)).alias(\"std\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    mean_val = stats[\"mean\"]\n",
    "    std_val = stats[\"std\"] if stats[\"std\"] != 0 else 1.0\n",
    "\n",
    "    scaled_col_name = f\"{c}_scaled\"\n",
    "    df_silver = df_silver.withColumn(\n",
    "        scaled_col_name,\n",
    "        (F.col(c) - F.lit(mean_val)) / F.lit(std_val)\n",
    "    )\n",
    "    scaled_cols.append(scaled_col_name)\n",
    "\n",
    "# -------------------------\n",
    "#  6. Seleccionar solo columnas escaladas + columnas string\n",
    "# -------------------------\n",
    "df_silver_scaled = df_silver.select(string_cols + scaled_cols + not_scaled_cols)\n",
    "\n",
    "# -------------------------\n",
    "# 7. Quitar filas con valores nulos\n",
    "# -------------------------\n",
    "df_silver_scaled = df_silver_scaled.na.drop()\n",
    "\n",
    "# -------------------------\n",
    "# 8️. Guardar la tabla en Delta como Silver\n",
    "# -------------------------\n",
    "# df_silver_scaled.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bank_loan.default.bank_personal_loan_silver\")\n",
    "# -------------------------\n",
    "# 9️. Mostrar resultados\n",
    "# -------------------------\n",
    "display(df_silver_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396ef187-16c8-49c2-a85c-c5e7a38f283a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver_scaled_pandas = df_silver_scaled.toPandas()\n",
    "df_silver_scaled_pandas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630bee16-d875-488e-8bcb-a7cd4369011c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Extraer la variable\n",
    "data = df_silver_scaled_pandas[\"age_scaled\"]\n",
    "\n",
    "# Calcular estadísticas\n",
    "media = np.mean(data)\n",
    "desv_std = np.std(data)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(data, bins=30, kde=True, color='skyblue')\n",
    "\n",
    "# Añadir líneas de media y desviación estándar\n",
    "plt.axvline(media, color='red', linestyle='--', label=f'Media = {media:.2f}')\n",
    "plt.axvline(media + desv_std, color='green', linestyle='--', label=f'Media + 1σ = {media + desv_std:.2f}')\n",
    "plt.axvline(media - desv_std, color='green', linestyle='--', label=f'Media - 1σ = {media - desv_std:.2f}')\n",
    "\n",
    "plt.xlabel(\"Income Escalado\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.title(\"Histograma de Income Escalado con Media y Desviación Estándar\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e2697d-2dc7-4f26-bad3-fa5c79433ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver_scaled_pandas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9457e909-c0ed-4570-8557-2cb7a8c79138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Capa Gold: Features finales para análisis y ML\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Se define la tabla Gold con la tabla Silver escalada\n",
    "df_gold = df_silver_scaled\n",
    "\n",
    "# -------------------------\n",
    "# 1️⃣ Crear flags y ratios usando solo columnas escaladas\n",
    "# -------------------------\n",
    "# Flag de alto ingreso (usando z-score)\n",
    "mean_income = df_gold.select(F.mean(\"income_scaled\")).collect()[0][0]\n",
    "df_gold = df_gold.withColumn(\"high_income_flag\", (F.col(\"income_scaled\") > mean_income).cast(\"int\"))\n",
    "\n",
    "# Ratio ingreso por miembro de la familia\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"income_per_family_member\",\n",
    "    F.col(\"income_scaled\") / F.col(\"family\")\n",
    ")\n",
    "\n",
    "# Flag de clientes jóvenes\n",
    "df_gold = df_gold.withColumn(\"young_flag\", (F.col(\"age_scaled\") < 0).cast(\"int\"))\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ Agregaciones por zip_code\n",
    "# -------------------------\n",
    "df_zip_agg = df_gold.groupBy(\"zip_code\").agg(\n",
    "    F.mean(\"ccavg_scaled\").alias(\"avg_ccavg_by_zip\"),\n",
    "    F.mean(\"income_scaled\").alias(\"avg_income_by_zip\")\n",
    ")\n",
    "\n",
    "# Hacer join de nuevo con df_gold\n",
    "df_gold = df_gold.join(df_zip_agg, on=\"zip_code\", how=\"left\")\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ Seleccionar columnas finales\n",
    "# -------------------------\n",
    "# Mantener ID, zip, binarios, escaladas, y nuevas features\n",
    "not_scaled_cols = [\n",
    "    'education',\n",
    "    'family',\n",
    "    'personal_loan',\n",
    "    'securities_account',\t\n",
    "    'cd_account',\t\n",
    "    'online',\t\n",
    "    'creditcard'\n",
    "]\n",
    "scaled_cols = [c for c in df_gold.columns if \"_scaled\" in c]\n",
    "new_features = [\"high_income_flag\", \"income_per_family_member\", \"young_flag\", \"avg_ccavg_by_zip\", \"avg_income_by_zip\"]\n",
    "\n",
    "final_cols = [\"id\", \"zip_code\"] + scaled_cols + not_scaled_cols + new_features\n",
    "df_gold_final = df_gold.select(final_cols)\n",
    "\n",
    "# -------------------------\n",
    "# 4️⃣ Guardar Gold en Delta\n",
    "# -------------------------\n",
    "# df_gold_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bank_loan.default.bank_personal_loan_gold\")\n",
    "\n",
    "# -------------------------\n",
    "# 5️⃣ Mostrar resultados\n",
    "# -------------------------\n",
    "display(df_gold_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a11caae3-335a-4f12-8cca-935ad3ca0240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Leer tabla Gold\n",
    "df_spark = spark.read.table(\"bank_loan.default.etl_bank_loan_gold\")\n",
    "\n",
    "# Convertir a pandas\n",
    "df = df_spark.toPandas()\n",
    "\n",
    "# Guardar id y zip_code\n",
    "ids = df[\"id\"]\n",
    "zips = df[\"zip_code\"]\n",
    "\n",
    "# Separar target y features automáticamente\n",
    "X = df.drop(columns=[\"id\", \"zip_code\", \"personal_loan\"])\n",
    "y = df[\"personal_loan\"]\n",
    "\n",
    "# Split train/test (mantener indices para unir id/zip)\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, df.index, test_size=0.2, random_state=848\n",
    ")\n",
    "\n",
    "# Entrenar Random Forest\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción y métrica\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Crear DataFrame final con id, zip_code, y_real y y_pred\n",
    "df_results = pd.DataFrame({\n",
    "    \"id\": ids.iloc[idx_test],\n",
    "    \"zip_code\": zips.iloc[idx_test],\n",
    "    \"y_real\": y_test,\n",
    "    \"y_pred\": y_pred\n",
    "})\n",
    "\n",
    "# Convertir a Spark DataFrame y guardar como Delta\n",
    "df_results_spark = spark.createDataFrame(df_results)\n",
    "## df_results_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bank_loan.default.rf_predictions_gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c99da6-328c-4d82-91be-645fd99b464f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Entrenar modelo\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=848,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Guardar con MLflow\n",
    "with mlflow.start_run():\n",
    "    mlflow.sklearn.log_model(model, \"personal_loan_model\")\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "banking_loan_nb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
